{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b46cc6bd",
   "metadata": {},
   "source": [
    "1. Explain the Activation Functions in your own language\n",
    "a) sigmoid\n",
    "b) tanh\n",
    "c) ReLU\n",
    "d) ELU\n",
    "e) LeakyReLU\n",
    "f) swish\n",
    "\n",
    "2. What happens when you increase or decrease the optimizer learning rate?\n",
    "\n",
    "3. What happens when you increase the number of internal hidden neurons?\n",
    "\n",
    "4. What happens when you increase the size of batch computation?\n",
    "\n",
    "5. Why we adopt regularization to avoid overfitting?\n",
    "\n",
    "6. What are loss and cost functions in deep learning?\n",
    "\n",
    "7. What do ou mean by underfitting in neural networks?\n",
    "\n",
    "8. Why we use Dropout in Neural Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7d8ec9",
   "metadata": {},
   "source": [
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06418637",
   "metadata": {},
   "source": [
    "1. Activation functions are used in neural networks to introduce nonlinearity into the outputs of each neuron. Here is a brief explanation of each activation function:\n",
    "a) Sigmoid: A sigmoid function maps the input to a value between 0 and 1, making it useful for binary classification problems. It can also cause the vanishing gradient problem, as its derivative becomes very small for large or small inputs.\n",
    "b) Tanh: The hyperbolic tangent function maps the input to a value between -1 and 1. It can be useful for classification tasks, but can also cause the vanishing gradient problem.\n",
    "c) ReLU: The Rectified Linear Unit function outputs the input directly if it is positive, and 0 if it is negative. ReLU is widely used due to its simplicity and effectiveness in deep networks, as it can help prevent the vanishing gradient problem.\n",
    "d) ELU: The Exponential Linear Unit function is similar to ReLU, but can produce negative output values, which can help prevent the dying ReLU problem where neurons become inactive and stop learning.\n",
    "e) LeakyReLU: A variant of ReLU, LeakyReLU allows for a small non-zero output for negative inputs, which can help prevent the dying ReLU problem.\n",
    "f) Swish: The Swish activation function is similar to ReLU, but applies a sigmoid-like function to the input, which can help produce more complex and accurate representations.\n",
    "\n",
    "2. Increasing the optimizer learning rate can lead to faster convergence during training, but may also cause the optimizer to overshoot the minimum and start oscillating around it, leading to slower convergence and a less stable optimization process. Decreasing the learning rate can help prevent overshooting and improve stability, but may also slow down the convergence rate.\n",
    "\n",
    "3. Increasing the number of internal hidden neurons can increase the complexity and expressiveness of the model, allowing it to learn more complex relationships and representations. However, increasing the number of neurons also increases the risk of overfitting, as the model may start to memorize the training data instead of generalizing to new data.\n",
    "\n",
    "4. Increasing the size of batch computation can lead to faster training times, as more training samples are processed in each iteration. However, larger batch sizes can also lead to worse generalization performance and slower convergence, as the model may start to memorize the training data instead of generalizing to new data.\n",
    "\n",
    "5. Regularization is used to avoid overfitting in neural networks by adding a penalty term to the loss function that discourages the weights from taking on large values. This can help prevent the model from memorizing the training data and instead encourages it to learn more generalizable representations.\n",
    "\n",
    "6. Loss functions are used to measure the difference between the predicted output of a neural network and the actual output. Cost functions are the average of the loss function over all training examples, and are used to evaluate the performance of the model during training.\n",
    "\n",
    "7. Underfitting in neural networks refers to a situation where the model is too simple to capture the underlying patterns in the data. This can result in high bias and low variance, and can lead to poor performance on both the training and test data.\n",
    "\n",
    "8. Dropout is used in neural networks to prevent overfitting by randomly dropping out some of the neurons during training. This forces the remaining neurons to learn more robust and generalizable representations, and can help improve the model's performance on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a683382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
