{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a59d6972",
   "metadata": {},
   "source": [
    "1. How does unsqueeze help us to solve certain broadcasting problems?\n",
    "2. How can we use indexing to do the same operation as unsqueeze?\n",
    "3. How do we show the actual contents of the memory used for a tensor?\n",
    "4. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added\n",
    "to each row or each column of the matrix? (Be sure to check your answer by running this\n",
    "code in a notebook.)\n",
    "5. Do broadcasting and expand_as result in increased memory use? Why or why not?\n",
    "6. Implement matmul using Einstein summation.\n",
    "7. What does a repeated index letter represent on the lefthand side of einsum?\n",
    "8. What are the three rules of Einstein summation notation? Why?\n",
    "9. What are the forward pass and backward pass of a neural network?\n",
    "10. Why do we need to store some of the activations calculated for intermediate layers in the\n",
    "forward pass?\n",
    "11. What is the downside of having activations with a standard deviation too far away from 1?\n",
    "12. How can weight initialization help avoid this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f155c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. `unsqueeze` helps us to add a new dimension to a tensor, which can help solve broadcasting problems by aligning the tensor dimensions correctly for arithmetic operations. For example, if we have a 1D tensor of size 3 and a 2D tensor of size 3x4, we can use `unsqueeze(0)` on the 1D tensor to create a new dimension of size 1, and then broadcast the tensor to match the size of the 2D tensor for element-wise operations.\n",
    "\n",
    "2. We can use indexing to achieve the same operation as `unsqueeze`. For example, if we have a tensor of size 2x3 and want to add a new dimension at axis 0, we can use indexing like `tensor[None, :, :]` to achieve the same result as `unsqueeze(0)`.\n",
    "\n",
    "3. To show the actual contents of the memory used for a tensor, we can use the `.storage()` method to get the underlying storage object and then use the `.tolist()` method to convert it to a list. For example, `tensor.storage().tolist()` will return a list of the elements in the tensor.\n",
    "\n",
    "4. When adding a vector of size 3 to a matrix of size 3x3, the elements of the vector are added to each column of the matrix. This is because broadcasting aligns the vector dimension with the column dimension of the matrix.\n",
    "\n",
    "5. `broadcasting` and `expand_as` do not necessarily result in increased memory use, as they do not create new copies of the tensor data. Instead, they create views of the existing data with different dimensions, and any actual memory allocation only happens if we modify the data.\n",
    "\n",
    "6. We can implement `matmul` using Einstein summation as follows: `torch.einsum(\"ij,jk->ik\", tensor1, tensor2)`.\n",
    "\n",
    "7. A repeated index letter on the lefthand side of `einsum` indicates a summation over that index. For example, if we have the expression `torch.einsum(\"ii->i\", tensor)`, this means we are summing over the diagonal elements of the tensor.\n",
    "\n",
    "8. The three rules of Einstein summation notation are:\n",
    "   - Repeated indices are summed over.\n",
    "   - Each index must appear exactly twice, once in each term of the product.\n",
    "   - The dimensions of the indices being multiplied must match.\n",
    "\n",
    "   These rules help ensure that the summation is well-defined and matches the intended mathematical operation.\n",
    "\n",
    "9. The `forward pass` of a neural network involves feeding input data through the network to obtain a prediction or output. The `backward pass` involves calculating the gradients of the loss with respect to the network parameters using the chain rule of differentiation, and using these gradients to update the parameters via an optimization algorithm like stochastic gradient descent.\n",
    "\n",
    "10. We need to store some of the activations calculated for intermediate layers in the forward pass because they are needed for the backward pass to calculate the gradients of the loss with respect to the parameters. Without these activations, we would need to recompute them during the backward pass, which would be computationally expensive.\n",
    "\n",
    "11. The downside of having activations with a standard deviation too far away from 1 is that it can lead to vanishing or exploding gradients during the backward pass. This can make it difficult or impossible to train the network effectively, as the gradients become too small or too large to update the parameters properly.\n",
    "\n",
    "12. Weight initialization can help avoid the problem of activations with a standard deviation too far away from 1 by setting the initial weights to appropriate values. For example, the Xavier initialization sets the weights based on the number of input and output units of a layer, which can help ensure that the activations have an appropriate variance. By choosing appropriate initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8bf392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
