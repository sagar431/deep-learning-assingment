{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccef7033",
   "metadata": {},
   "source": [
    "1. Write the Python code to implement a single neuron.\n",
    "2. Write the Python code to implement ReLU.\n",
    "3. Write the Python code for a dense layer in terms of matrix multiplication.\n",
    "4. Write the Python code for a dense layer in plain Python (that is, with list comprehensions\n",
    "and functionality built into Python).\n",
    "5. What is the “hidden size” of a layer?\n",
    "6. What does the t method do in PyTorch?\n",
    "7. Why is matrix multiplication written in plain Python very slow?\n",
    "8. In matmul, why is ac==br?\n",
    "9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
    "10. What is elementwise arithmetic?\n",
    "11. Write the PyTorch code to test whether every element of a is greater than the\n",
    "corresponding element of b.\n",
    "12. What is a rank-0 tensor? How do you convert it to a plain Python data type?\n",
    "13. How does elementwise arithmetic help us speed up matmul?\n",
    "14. What are the broadcasting rules?\n",
    "15. What is expand_as? Show an example of how it can be used to match the results of\n",
    "broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e353bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        self.weights = np.random.randn(input_size)\n",
    "        self.bias = np.random.randn()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        z = np.dot(inputs, self.weights) + self.bias\n",
    "        output = self.activation(z)\n",
    "        return output\n",
    "    \n",
    "    def activation(self, x):\n",
    "        return x # identity activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d541772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.def relu(x):\n",
    "    return np.maximum(0, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f5d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.import torch\n",
    "\n",
    "class DenseLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.weights = torch.nn.Parameter(torch.randn(input_size, output_size))\n",
    "        self.bias = torch.nn.Parameter(torch.randn(output_size))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output = torch.matmul(inputs, self.weights) + self.bias\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98043ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.def dense_layer(inputs, weights, bias):\n",
    "    outputs = [sum(x*w for x,w in zip(inputs, weights_row)) + bias_row for weights_row, bias_row in zip(weights, bias)]\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7515888",
   "metadata": {},
   "source": [
    "5.The hidden size of a layer refers to the number of neurons in the layer. It determines the capacity of the layer to learn complex representations of the input data.\n",
    "\n",
    "6.The t method in PyTorch is used to transpose a tensor. It flips the dimensions of the tensor along the diagonal.\n",
    "\n",
    "7.Matrix multiplication written in plain Python is slow because it involves nested loops over the elements of the matrices, which can be very computationally expensive for large matrices. In addition, Python lists and loops are not optimized for numerical operations, which can further slow down the computation.\n",
    "\n",
    "8.In matmul, ac==br because the number of columns in the first matrix (a) must match the number of rows in the second matrix (b) in order for matrix multiplication to be defined.\n",
    "\n",
    "9.In Jupyter Notebook, you can measure the time taken for a single cell to execute using the %timeit magic command. Simply add %timeit before the code you want to measure, and Jupyter Notebook will run the code several times and report the average execution time.\n",
    "\n",
    "10.Elementwise arithmetic refers to arithmetic operations that are performed on each element of a tensor independently. Examples include adding a scalar to a tensor, multiplying two tensors elementwise, or applying a function to each element of a tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc61df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "11.a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([0, 2, 4])\n",
    "result = torch.all(a > b)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2dc4bf",
   "metadata": {},
   "source": [
    "12.A rank-0 tensor is a scalar tensor, i.e., a tensor with no dimensions other than the single element it contains. To convert it to a plain Python data type, you can use the .item() method.\n",
    "\n",
    "13.Elementwise arithmetic helps speed up matmul by allowing us to perform some operations (such as adding bias terms or applying activation functions) without explicitly computing the full matrix multiplication. Instead, we can apply these operations directly to the elements of the tensors involved, which can be more efficient.\n",
    "\n",
    "14.The broadcasting rules define how tensors of different shapes can be combined in elementwise operations. They allow us to perform operations on tensors with different shapes by expanding or duplicating the smaller tensor along one or more dimensions to match the shape of the larger tensor.\n",
    "\n",
    "15.expand_as is a PyTorch method that expands the dimensions of a tensor to match the dimensions of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f641a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
