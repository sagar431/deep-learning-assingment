{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3664faa",
   "metadata": {},
   "source": [
    "1. Is it okay to initialize all the weights to the same value as long as that value is selected\n",
    "randomly using He initialization?\n",
    "2. Is it okay to initialize the bias terms to 0?\n",
    "3. Name three advantages of the ELU activation function over ReLU.\n",
    "4. In which cases would you want to use each of the following activation functions: ELU, leaky\n",
    "ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999)\n",
    "when using a MomentumOptimizer?\n",
    "6. Name three ways you can produce a sparse model.\n",
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on\n",
    "new instances)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29308aa",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bce9a1",
   "metadata": {},
   "source": [
    "1. It is not okay to initialize all the weights to the same value, even if the value is selected randomly using He initialization. This is because the symmetry of the weights can cause the gradients to vanish or explode, leading to slow or unstable learning. Instead, the weights should be initialized randomly using a distribution that preserves the variance of the activations.\n",
    "\n",
    "2. It is generally okay to initialize the bias terms to 0, as long as the weights are initialized properly. However, some researchers suggest initializing the bias terms to a small positive value, such as 0.1, to introduce some level of symmetry breaking.\n",
    "\n",
    "3. Three advantages of the ELU (Exponential Linear Unit) activation function over ReLU (Rectified Linear Unit) are:\n",
    "- ELU can produce negative values, which can help prevent the dying ReLU problem where neurons become inactive and stop learning.\n",
    "- ELU has a smooth non-zero gradient for both positive and negative inputs, which can help speed up learning and reduce the likelihood of vanishing gradients.\n",
    "- ELU can produce smaller output values, which can help improve the robustness of the model to noise and outliers.\n",
    "\n",
    "4. The activation function used in a neural network depends on the task and the architecture of the network. Here are some guidelines for selecting activation functions:\n",
    "- ELU or leaky ReLU can be useful for deep networks to prevent the vanishing gradient problem.\n",
    "- ReLU is a good default choice for most situations, as it is simple and effective.\n",
    "- Tanh or logistic functions can be useful for binary classification problems, where the output should be between 0 and 1.\n",
    "- Softmax is typically used in the output layer of classification networks, where the outputs should represent probabilities.\n",
    "\n",
    "5. If the momentum hyperparameter is set too close to 1 when using a MomentumOptimizer, it can cause the optimizer to overshoot the minimum and start oscillating around it. This can lead to slower convergence and a less stable optimization process.\n",
    "\n",
    "6. Three ways to produce a sparse model are:\n",
    "- L1 regularization: penalizing the absolute values of the weights can encourage the model to set some weights to 0.\n",
    "- Dropout: randomly setting some neurons to 0 during training can encourage the model to rely on a subset of the neurons.\n",
    "- Pruning: removing weights or entire neurons that are deemed unimportant based on some criteria, such as their magnitude or contribution to the overall loss.\n",
    "\n",
    "7. Dropout can slow down training because it introduces randomness and noise to the gradients, requiring more iterations to converge. However, it can also improve the generalization performance of the model by reducing overfitting. Dropout does not slow down inference, as it is only applied during training and not during prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26dc4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
