{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1898912e",
   "metadata": {},
   "source": [
    "1. Why is it generally preferable to use a Logistic Regression classifier rather than a classical\n",
    "Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training\n",
    "algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression\n",
    "classifier?\n",
    "2. Why was the logistic activation function a key ingredient in training the first MLPs?\n",
    "3. Name three popular activation functions. Can you draw them?\n",
    "4. Suppose you have an MLP composed of one input layer with 10 passthrough neurons,\n",
    "followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3\n",
    "artificial neurons. All artificial neurons use the ReLU activation function.\n",
    " What is the shape of the input matrix X?\n",
    " What about the shape of the hidden layer’s weight vector Wh, and the shape of its\n",
    "bias vector bh?\n",
    " What is the shape of the output layer’s weight vector Wo, and its bias vector bo?\n",
    " What is the shape of the network’s output matrix Y?\n",
    " Write the equation that computes the network’s output matrix Y as a function\n",
    "of X, Wh, bh, Wo and bo.\n",
    "\n",
    "5. How many neurons do you need in the output layer if you want to classify email into spam\n",
    "or ham? What activation function should you use in the output layer? If instead you want to\n",
    "tackle MNIST, how many neurons do you need in the output layer, using what activation\n",
    "function?\n",
    "6. What is backpropagation and how does it work? What is the difference between\n",
    "backpropagation and reverse-mode autodiff?\n",
    "7. Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the\n",
    "training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "8. Train a deep MLP on the MNIST dataset and see if you can get over 98% precision. Try\n",
    "adding all the bells and whistles (i.e., save checkpoints, restore the last checkpoint in case of\n",
    "an interruption, add summaries, plot learning curves using TensorBoard, and so on)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6511557e",
   "metadata": {},
   "source": [
    "# ANs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5847771f",
   "metadata": {},
   "source": [
    "1. Logistic Regression is generally preferable to a classical Perceptron because it can output class probabilities, while a Perceptron can only output binary classifications. Perceptron uses a step function to classify inputs as either 0 or 1, while Logistic Regression uses a sigmoid function to output a probability between 0 and 1. To make a Perceptron equivalent to a Logistic Regression classifier, the Perceptron's step function can be replaced with the sigmoid function.\n",
    "\n",
    "2. The logistic activation function was a key ingredient in training the first MLPs because it is differentiable, allowing for efficient use of backpropagation to update the weights of the network during training. Additionally, the logistic function has a smooth gradient, allowing for more stable and consistent learning.\n",
    "\n",
    "3. Three popular activation functions are: \n",
    "- ReLU (Rectified Linear Unit)\n",
    "- Sigmoid \n",
    "- Tanh (Hyperbolic tangent)\n",
    "\n",
    "4. \n",
    "- The shape of the input matrix X is (n_samples, 10), where n_samples is the number of input samples.\n",
    "- The shape of the hidden layer's weight vector Wh is (10, 50), where 10 is the number of input features and 50 is the number of neurons in the hidden layer.\n",
    "- The shape of the hidden layer's bias vector bh is (50,), with one bias term for each neuron in the hidden layer.\n",
    "- The shape of the output layer's weight vector Wo is (50, 3), where 50 is the number of neurons in the hidden layer and 3 is the number of output classes.\n",
    "- The shape of the output layer's bias vector bo is (3,), with one bias term for each output class.\n",
    "- The shape of the network's output matrix Y is (n_samples, 3), where n_samples is the number of input samples.\n",
    "\n",
    "- The equation that computes the network's output matrix Y as a function of X, Wh, bh, Wo, and bo is:\n",
    "Y = relu(X.dot(Wh) + bh).dot(Wo) + bo\n",
    "\n",
    "5. For email classification into spam or ham, you only need one neuron in the output layer with the sigmoid activation function. For MNIST, you need 10 neurons in the output layer (one for each digit) with the softmax activation function.\n",
    "\n",
    "6. Backpropagation is an algorithm used to update the weights of a neural network during training. It works by computing the gradient of the loss function with respect to the weights of the network, and then using this gradient to update the weights using gradient descent. Reverse-mode autodiff is an alternative method to compute gradients that is more efficient and accurate than computing gradients using finite differences. Reverse-mode autodiff is based on the chain rule of calculus, and it allows for efficient computation of gradients in deep neural networks.\n",
    "\n",
    "7. Some hyperparameters that can be tweaked in an MLP include: the number of layers, the number of neurons in each layer, the learning rate, the activation function used in each layer, and the regularization strength. If the MLP overfits the training data, the hyperparameters that can be adjusted to solve the problem include: decreasing the number of neurons in each layer, increasing the regularization strength, and decreasing the learning rate.\n",
    "\n",
    "8. Training a deep MLP on the MNIST dataset with over 98% precision can be achieved by using a combination of techniques such as dropout regularization, batch normalization, and early stopping. Additionally, using more advanced optimization algorithms such as Adam or RMSprop can also help improve performance. Saving checkpoints, restoring the last checkpoint in case of an interruption, adding summaries, and plotting learning curves using TensorBoard can also be helpful in monitoring the progress of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad1cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
