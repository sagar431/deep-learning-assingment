{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec55ae18",
   "metadata": {},
   "source": [
    "1. What does a SavedModel contain? How do you inspect its content?\n",
    "2. When should you use TF Serving? What are its main features? What are some tools you can\n",
    "use to deploy it?\n",
    "3. How do you deploy a model across multiple TF Serving instances?\n",
    "4. When should you use the gRPC API rather than the REST API to query a model served by TF\n",
    "Serving?\n",
    "5. What are the different ways TFLite reduces a model’s size to make it run on a mobile or\n",
    "embedded device?\n",
    "6. What is quantization-aware training, and why would you need it?\n",
    "7. What are model parallelism and data parallelism? Why is the latter\n",
    "generally recommended?\n",
    "8. When training a model across multiple servers, what distribution strategies can you use?\n",
    "How do you choose which one to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739817b",
   "metadata": {},
   "source": [
    "1. A SavedModel is a format for saving and loading TensorFlow models. It contains the graph definition, the trained weights, and the metadata required to run the model. You can inspect the content of a SavedModel using the `saved_model_cli` tool, which allows you to view the signature of the model, the input and output tensors, and the version information.\n",
    "\n",
    "2. TF Serving is a system for serving machine learning models in a scalable and efficient manner. It is designed to handle high traffic loads and to provide low-latency responses to client requests. You should use TF Serving when you need to serve machine learning models in production, and when you need to update the models frequently without interrupting the serving process. TF Serving's main features include support for multiple models, dynamic model loading, and configurable batching and caching. Some tools you can use to deploy TF Serving include Kubernetes, Docker, and Helm.\n",
    "\n",
    "3. To deploy a model across multiple TF Serving instances, you can use a load balancer to distribute the requests among the instances. You can also use replicas to provide fault tolerance and ensure high availability. TF Serving supports various deployment configurations, including standalone, replicated, and distributed.\n",
    "\n",
    "4. The gRPC API is generally preferred over the REST API to query a model served by TF Serving because it offers better performance and supports streaming and bidirectional communication. The REST API, on the other hand, is simpler to use and more widely supported by client libraries and frameworks.\n",
    "\n",
    "5. TFLite reduces a model's size to make it run on a mobile or embedded device in several ways. These include quantization (reducing the precision of the weights and activations), pruning (removing unnecessary weights and connections), and distillation (training a smaller model to mimic the behavior of a larger model). TFLite also provides hardware acceleration using specialized hardware such as GPUs, DSPs, and NPUs.\n",
    "\n",
    "6. Quantization-aware training is a technique used to train models that can be efficiently quantized for deployment on mobile or embedded devices. It involves simulating the effects of quantization during training, by introducing noise or regularization to the model's weights and activations. This allows the model to learn weights that are more resilient to quantization, resulting in better accuracy when the model is actually quantized.\n",
    "\n",
    "7. Model parallelism and data parallelism are two techniques used to parallelize the training of large models across multiple devices or machines. Model parallelism involves splitting the model across multiple devices, with each device responsible for computing a portion of the model's forward and backward passes. Data parallelism, on the other hand, involves replicating the model across multiple devices, with each device processing a different batch of data in parallel. Data parallelism is generally recommended because it is easier to implement and can scale to large clusters.\n",
    "\n",
    "8. When training a model across multiple servers, you can use various distribution strategies such as synchronous training, asynchronous training, and parameter server training. The choice of distribution strategy depends on factors such as the size of the model, the amount of data, the network bandwidth, and the availability of hardware. Synchronous training is generally preferred for small to medium-sized models, while asynchronous training and parameter server training are more suitable for large models with many parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b77637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
