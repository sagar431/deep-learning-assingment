{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.What is the function of a summation junction of a neuron? What is threshold activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summation, in physiology, the additive effect of several electrical impulses on a neuromuscular junction, the junction between a nerve cell and a muscle cell. Individually the stimuli cannot evoke a response, but collectively they can generate a response.\n",
    "Binary Step Activation Function. Binary step function is a threshold-based activation function which means after a certain threshold neuron is activated and below the said threshold neuron is deactivated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.What is a step function? What is the difference of step function with threshold function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematics, a function on the real numbers is called a step function (or staircase function) if it can be written as a finite linear combination of indicator functions of intervals. Informally speaking, a step function is a piecewise constant function having only finitely many pieces.\n",
    "What is meant by activation function and threshold function?\n",
    "An activation function is a function used in artificial neural networks which outputs a small value for small inputs, and a larger value if its inputs exceed a threshold. ... Activation functions are useful because they add non-linearities into neural networks, allowing the neural networks to learn powerful operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Explain the McCulloch–Pitts model of neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The McCulloch-Pitts neural model, which was the earliest ANN model, has only two types of inputs — Excitatory and Inhibitory. The excitatory inputs have weights of positive magnitude and the inhibitory weights have weights of negative magnitude. The inputs of the McCulloch-Pitts neuron could be either 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Explain the ADALINE network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " MADALINE (Many ADALINE) is a three-layer (input, hidden, output), fully connected, feed-forward artificial neural network architecture for classification that uses ADALINE units in its hidden and output layers, i.e. its activation function is the sign function. The three-layer network uses memistors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.What is the constraint of a simple perceptron? Why it may fail with a real-world data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons were able to solve a range of decision problems, in particular they were able to represent logic gates such as “AND”, “OR,” and “NOT.” The perceptron learning rule tended to converge to an optimal set of weights for several classes of input patterns. However, this was not always guaranteed. Another limitation arose when the data were not linearly separable—for example, the classic “XOR.” A XOR gate resolves to “true” if the two inputs are different and resolves to “false” if both inputs are the same (Fig. 10.3). Minsky and Papert published these and other core limitations of perceptrons in a 1969 book called Perceptrons, which arguably reduced further interest in these types of ANN and the so-called AI Winter had set in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.What is linearly inseparable problem? What is the role of the hidden layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly not all decision problems are linearly separable: they cannot be solved using a linear decision boundary. Problems like these are termed linearly inseparable.\n",
    "The role of the Hidden Layers is to identify features from the input data and use these to correlate between a given input and the correct output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Explain XOR problem in case of a simple perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"single-layer\" perceptron can't implement XOR. The reason is because the classes in XOR are not linearly separable. You cannot draw a straight line to separate the points (0,0),(1,1) from the points (0,1),(1,0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.Design a multi-layer perceptron to implement A XOR B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR(0, 1) = 1\n",
      "XOR(1, 1) = 0\n",
      "XOR(0, 0) = 0\n",
      "XOR(1, 0) = 1\n"
     ]
    }
   ],
   "source": [
    "# importing Python library\n",
    "import numpy as np\n",
    "\n",
    "# define Unit Step Function\n",
    "def unitStep(v):\n",
    "    if v >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# design Perceptron Model\n",
    "def perceptronModel(x, w, b):\n",
    "    v = np.dot(w, x) + b\n",
    "    y = unitStep(v)\n",
    "    return y\n",
    "\n",
    "# NOT Logic Function\n",
    "# wNOT = -1, bNOT = 0.5\n",
    "def NOT_logicFunction(x):\n",
    "    wNOT = -1\n",
    "    bNOT = 0.5\n",
    "    return perceptronModel(x, wNOT, bNOT)\n",
    "\n",
    "# AND Logic Function\n",
    "# here w1 = wAND1 = 1,\n",
    "# w2 = wAND2 = 1, bAND = -1.5\n",
    "def AND_logicFunction(x):\n",
    "    w = np.array([1, 1])\n",
    "    bAND = -1.5\n",
    "    return perceptronModel(x, w, bAND)\n",
    "\n",
    "# OR Logic Function\n",
    "# w1 = 1, w2 = 1, bOR = -0.5\n",
    "def OR_logicFunction(x):\n",
    "    w = np.array([1, 1])\n",
    "    bOR = -0.5\n",
    "    return perceptronModel(x, w, bOR)\n",
    "\n",
    "# XOR Logic Function\n",
    "# with AND, OR and NOT\n",
    "# function calls in sequence\n",
    "def XOR_logicFunction(x):\n",
    "    y1 = AND_logicFunction(x)\n",
    "    y2 = OR_logicFunction(x)\n",
    "    y3 = NOT_logicFunction(y1)\n",
    "    final_x = np.array([y2, y3])\n",
    "    finalOutput = AND_logicFunction(final_x)\n",
    "    return finalOutput\n",
    "\n",
    "# testing the Perceptron Model\n",
    "test1 = np.array([0, 1])\n",
    "test2 = np.array([1, 1])\n",
    "test3 = np.array([0, 0])\n",
    "test4 = np.array([1, 0])\n",
    "\n",
    "print(\"XOR({}, {}) = {}\".format(0, 1, XOR_logicFunction(test1)))\n",
    "print(\"XOR({}, {}) = {}\".format(1, 1, XOR_logicFunction(test2)))\n",
    "print(\"XOR({}, {}) = {}\".format(0, 0, XOR_logicFunction(test3)))\n",
    "print(\"XOR({}, {}) = {}\".format(1, 0, XOR_logicFunction(test4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.Explain the single-layer feed forward architecture of ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Layer Feed Forward Network\n",
    "Output Layer is formed when different weights are applied on input nodes and the cumulative effect per node is taken. After this, the neurons collectively give the output layer to compute the output signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.Explain the competitive network architecture of ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Competitive learning is a form of unsupervised learning in artificial neural networks, in which nodes compete for the right to respond to a subset of the input data. A variant of Hebbian learning, competitive learning works by increasing the specialization of each node in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.Consider a multi-layer feed forward neural network. Enumerate and explain steps in the backpropagation algorithm used to train the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While designing a Neural Network, in the beginning, we initialize weights with some random values or any variable for that fact.\n",
    "\n",
    "Now obviously, we are not superhuman. So, it’s not necessary that whatever weight values we have selected will be correct, or it fits our model the best.\n",
    "\n",
    "Okay, fine, we have selected some weight values in the beginning, but our model output is way different than our actual output i.e. the error value is huge.\n",
    "\n",
    "Now, how will you reduce the error?\n",
    "\n",
    "Basically, what we need to do, we need to somehow explain the model to change the parameters (weights), such that error becomes minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.What are the advantages and disadvantages of neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network problem does not immediately corrode. Ability to train machine: Artificial neural networks learn events and make decisions by commenting on similar events. Parallel processing ability: Artificial neural networks have numerical strength that can perform more than one job at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.Write short notes on any two of the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Biological neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biological neuron models, also known as a spiking neuron models,[1] are mathematical descriptions of the properties of certain cells in the nervous system that generate sharp electrical potentials across their cell membrane, roughly one millisecond in duration, called action potentials or spikes (Fig. 2). Since spikes are transmitted along the axon and synapses from the sending neuron to many other neurons, spiking neurons are considered to be a major information processing unit of the nervous system. Spiking neuron models can be divided into different categories: the most detailed mathematical models are biophysical neuron models (also called Hodgkin-Huxley models) that describe the membrane voltage as a function of the input current and the activation of ion channels. Mathematically simpler are integrate-and-fire models that describe the membrane voltage as a function of the input current and predict the spike times without a description of the biophysical processes that shape the time course of an action potential. Even more abstract models only predict output spikes (but not membrane voltage) as a function of the stimulation where the stimulation can occur through sensory input or pharmacologically. This article provides a short overview of different spiking neuron models and links, whenever possible to experimental phenomena. It includes deterministic and probabilistic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.ReLU function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In a neural network, the activation function is responsible for transforming the summed weighted input from the node into the activation of the node or output for that input.\n",
    "\n",
    "The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Single-layer feed forward ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this type of network, we have only two layers input layer and output layer but the input layer does not count because no computation is performed in this layer. The output layer is formed when different weights are applied on input nodes and the cumulative effect per node is taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent (GD) is an iterative first-order optimisation algorithm used to find a local minimum/maximum of a given function. This method is commonly used in machine learning (ML) and deep learning(DL) to minimise a cost/loss function (e.g. in a linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.Recurrent networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent neural network (RNN) is a type of artificial neural network which uses sequential data or time series data. These deep learning algorithms are commonly used for ordinal or temporal problems, such as language translation, natural language processing (nlp), speech recognition, and image captioning; they are incorporated into popular applications such as Siri, voice search, and Google Translate. Like feedforward and convolutional neural networks (CNNs), recurrent neural networks utilize training data to learn. They are distinguished by their “memory” as they take information from prior inputs to influence the current input and output. While traditional deep neural networks assume that inputs and outputs are independent of each other, the output of recurrent neural networks depend on the prior elements within the sequence. While future events would also be helpful in determining the output of a given sequence, unidirectional recurrent neural networks cannot account for these events in their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
